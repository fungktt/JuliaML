{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the efficiency of the TextAnalysis.jl Sentiment Analysis model\n",
    "\n",
    "### The full code is on [this GitHubGist](https://gist.github.com/fungktt/a826c6e778f9485d4632e555212a1b85#file-sentiment-analysis-jl) file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "using TextAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "setup (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function setup()\n",
    "    #opening the txt file containing all the information, and automatically closes it when done\n",
    "    open(\"amazonreviews/test.ft.txt\") do file\n",
    "        \n",
    "        #gathering all the data from the txt file\n",
    "        global mydata = readlines(file)        \n",
    "    end\n",
    "    \n",
    "    #defining the pre-trained model to be used\n",
    "    global model = SentimentAnalyzer()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment Analysis Model Trained on IMDB with a 88587 word corpus"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "analysis (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function analysis()\n",
    "    #defining two counters which will count the number of reviews analysed and number of correct analysis respectively\n",
    "    iterations = 0\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    \n",
    "    #for loop for each line in the txt file\n",
    "    for num in 1:length(mydata)\n",
    "        \n",
    "        #try/catch loop to catch out errors, explained later on\n",
    "        try\n",
    "            \n",
    "            #getting the sentiment value from the txt file, 2 = positive, 1 = negative\n",
    "            sentiment = parse(Int64,string(mydata[num][10])) \n",
    "            \n",
    "            #getting the review as a string\n",
    "            sd = StringDocument(mydata[num][12:end])\n",
    "            \n",
    "            #removing corrupted characters\n",
    "            remove_corrupt_utf8!(sd)\n",
    "            \n",
    "            #plugging the review into the model to get a floating point number ranging from 0-1, 1 = positive, 0 = negative\n",
    "            value = model(sd)\n",
    "            \n",
    "            #adding one to the number of reviews analysed, to be displayed at the end\n",
    "            iterations = iterations + 1 \n",
    "            \n",
    "            #when both are positive (ie. analysed correctly), add one to the number of correct interpretations\n",
    "            if value > 0.5 && sentiment == 2\n",
    "                tp = tp + 1\n",
    "                \n",
    "            #when both are negative (ie. analysed correctly), add one to the number of correct interpretations\n",
    "            elseif value < 0.5 && sentiment == 1\n",
    "                tn = tn + 1\n",
    "                \n",
    "            elseif value > 0.5 && sentiment == 1\n",
    "                fp = fp +1\n",
    "                \n",
    "            elseif value < 0.5 && sentiment == 2\n",
    "                fn = fn +1\n",
    "                \n",
    "            end\n",
    "            \n",
    "        #catches out the BoundsError problem\n",
    "        catch\n",
    "            \n",
    "            #skips the rest of the iteration to ignore the review and move onto the next (by the for loop)\n",
    "            continue\n",
    "        end\n",
    "    end\n",
    "    println(\"True Positive: \", tp)\n",
    "    println(\"True Negative: \", tn)\n",
    "    println(\"False Positive: \", fp)\n",
    "    println(\"False Negative: \", fn)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive: 118276\n",
      "True Negative: 88827\n",
      "False Positive: 110979\n",
      "False Negative: 81621\n"
     ]
    }
   ],
   "source": [
    "analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis:\n",
    "\n",
    "[This blog](https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226) gave me an insight on the different metrics we can use to measure the efficiency of our sentiment analysis model. <br>\n",
    "Also, thanks must go to PseudoCodeNerd for introducing this to me! :)\n",
    "\n",
    "**Accuracy**: the proportion of true results among the total number of cases examined. <br>\n",
    "    (TP+TN)/(TP+FP+FN+TN) = 207103/399703 = **0.5181422206**\n",
    "    \n",
    "**Precision**: the proportion of results that were actually positive among those that were predicted as positive. <br>\n",
    "    (TP)/(TP+FP) = 118276/229255 = **0.5159145929**\n",
    "    \n",
    "**Recall**: the proportion of results that were predicted as positive amone those that were actually positive. <br>\n",
    "    (TP)/(TP+FN) = 118276/199897 = **0.5916847176**\n",
    "    \n",
    "**F1 Score**: a number between 0 and 1 that is the harmonic mean of precision and recall. <br>\n",
    "    2(Precision * Recall)/(Precision + Recall) = **0.5512079636**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
